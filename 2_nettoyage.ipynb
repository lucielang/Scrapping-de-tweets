{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage des données\n",
    "\n",
    "Après avoir scrappé, il nous faut tenir compte de plusieurs choses pour nettoyer les bases de données et les fusionner :\n",
    "- dans un premier temps, on convertit les dates dans un format date permettant ensuite de trier et de réarranger la table dans le bon ordre temporel.\n",
    "- les cases vides susceptibles d'apparaître dans les colonnes \"Likes\", \"Views\", etc... sont remplacées par des 0 (il n'y a eu en effet aucun like par exemple).\n",
    "- on fusionne les tables et on les trie par date\n",
    "- enfin, on supprime les bots, définis dans notre code par une répétition de 4 tweets faisant moins de 1000 vues (car certains tweets qui se répètent peuvent être des titres d'article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting datetime\n",
      "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting zope.interface (from datetime)\n",
      "  Downloading zope.interface-7.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from datetime) (2024.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from zope.interface->datetime) (75.6.0)\n",
      "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
      "Downloading zope.interface-7.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "Installing collected packages: zope.interface, datetime\n",
      "Successfully installed datetime-5.5 zope.interface-7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas \n",
    "!pip install datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convertir les dates en format date\n",
    "\n",
    "Dans le code suivant, nous cherchons d'une part à remplacer les 2.3K par 2300, à convertir les dates dans un format *datetime*.\n",
    "\n",
    "Nous avons fait le choix d'ajouter deux colonnes :\n",
    "- une colonne YearWeek sous format 2024-1 pour la première semaine de 2024\n",
    "- une colonne YearMonth sous format 2024-10 pour le mois d'octobre 2024\n",
    "\n",
    "Nous avons décidé de nous restreindre à l'année 2024, en excluant le mois de décembre (car pas complet) : en effet, les fluctuations de 2023 étaient relativement similaires à celles de début de début 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_1.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_2.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_2.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_3.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_4.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_5.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_6.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_leave_p1.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_quit.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/bluesky.xlsx\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Boucle principale pour traiter chaque fichier\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraitement du fichier : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Charger les données\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Nettoyer les colonnes numériques\u001b[39;00m\n\u001b[1;32m     57\u001b[0m colonnes_a_nettoyer \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRepost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLikes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViews\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:1419\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peek\u001b[38;5;241m.\u001b[39mstartswith(ZIP_SIGNATURE):\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zf:\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;66;03m# Workaround for some third party files that use forward slashes and\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;66;03m# lower case names.\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m     component_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1423\u001b[0m         name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m   1424\u001b[0m     ]\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxl/workbook.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m component_names:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/zipfile/__init__.py:1349\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1349\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/zipfile/__init__.py:1416\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[0;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# Fonction 1 : Nettoyer les données numériques\n",
    "def convertir_en_nombre(valeur):\n",
    "    \"\"\"\n",
    "    Convertit les chaînes de caractères (ex: '1.2K') en nombres.\n",
    "    Remplace NaN par 0.\n",
    "    \"\"\"\n",
    "    if pd.isna(valeur):  # Si la valeur est NaN, retourne 0\n",
    "        return 0\n",
    "    if isinstance(valeur, str):  # Vérifie si la valeur est une chaîne de caractères\n",
    "        valeur = valeur.strip()  # Supprime les espaces inutiles\n",
    "        if 'K' in valeur:  # Si la valeur contient 'K', multiplier par 1 000\n",
    "            return float(valeur.replace('K', '')) * 1000\n",
    "        elif 'M' in valeur:  # Si la valeur contient 'M', multiplier par 1 000 000\n",
    "            return float(valeur.replace('M', '')) * 1000000\n",
    "    try:\n",
    "        return float(valeur)  # Sinon, convertir directement en nombre\n",
    "    except ValueError:\n",
    "        return 0  # Si la conversion échoue, retourner 0\n",
    "\n",
    "# Fonction 2 : Conversion des dates\n",
    "def convert_date(value, previous_date):\n",
    "    \"\"\"\n",
    "    Convertit les valeurs de date relative ou absolue en objets datetime.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = str(value).strip()  # S'assurer que la valeur est une chaîne\n",
    "        \n",
    "        # Cas 1 : Format \"Nov 5\" (pas d'année explicite)\n",
    "        if len(value.split()) == 2:\n",
    "            value_with_year = value + \" 2024\"  # Ajouter l'année par défaut\n",
    "            return datetime.strptime(value_with_year, '%b %d %Y')\n",
    "        \n",
    "        # Cas 2 : Format complet \"Nov 5, 2023\"\n",
    "        elif len(value.split()) == 3:\n",
    "            return datetime.strptime(value, '%b %d, %Y')\n",
    "        \n",
    "        # Cas 3 : Valeur non reconnue\n",
    "        else:\n",
    "            return pd.NaT\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la conversion de la date : {value} -> {e}\")\n",
    "        return pd.NaT  # Retourne NaT si la conversion échoue\n",
    "\n",
    "# Fonction 3 : Nettoyer et transformer un fichier\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Nettoie et transforme un fichier Excel, en ajoutant des colonnes converties\n",
    "    et en générant deux fichiers de sortie :\n",
    "    1. Toutes les données transformées.\n",
    "    2. Données filtrées avant une date spécifique.\n",
    "    \"\"\"\n",
    "    print(f\"Traitement du fichier : {file_path}\")\n",
    "    # Charger les données\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Nettoyer les colonnes numériques\n",
    "    colonnes_a_nettoyer = ['Comments', 'Repost', 'Likes', 'Views']\n",
    "    for colonne in colonnes_a_nettoyer:\n",
    "        if colonne in df.columns:\n",
    "            df[colonne] = df[colonne].apply(convertir_en_nombre)\n",
    "    \n",
    "    # Convertir les dates\n",
    "    converted_dates = []\n",
    "    previous_date = None\n",
    "    for value in df['Date']:\n",
    "        if previous_date is None:  # La première ligne doit être une date absolue\n",
    "            converted_date = convert_date(value, datetime(2024, 1, 1))  # Supposition du début\n",
    "        else:\n",
    "            converted_date = convert_date(value, previous_date)\n",
    "        converted_dates.append(converted_date)\n",
    "        if pd.notna(converted_date):  # Mettre à jour la référence seulement si la conversion a réussi\n",
    "            previous_date = converted_date\n",
    "    \n",
    "    # Ajouter les colonnes converties\n",
    "    df['ConvertedDate'] = converted_dates\n",
    "    df['YearWeek'] = df['ConvertedDate'].dt.strftime('%Y-%U')\n",
    "    df['YearMonth'] = df['ConvertedDate'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    # Filtrer les données pour garder uniquement les dates entre le 1er janv et le 1er décembre 2024\n",
    "    # Définir les bornes de la période\n",
    "    start_date = datetime(2024, 1, 1)  # 1er janvier 2024 inclus\n",
    "    end_date = datetime(2024, 11, 30)  # 30 novembre 2024 inclus\n",
    "\n",
    "    # Appliquer le filtre pour garder les dates entre ces bornes\n",
    "    df_filtered = df[(df['ConvertedDate'] >= start_date) & (df['ConvertedDate'] <= end_date)]\n",
    "\n",
    "\n",
    "    # Chemin vers le nouveau dossier\n",
    "    output_folder = '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye'\n",
    "\n",
    "    # Générer le chemin du fichier de sortie : on renomme les fichiers en ajoutant _date à la fin\n",
    "    base_filename = os.path.basename(file_path).replace('.xlsx', '_date.xlsx')\n",
    "    output_file_filtered = os.path.join(output_folder, base_filename)\n",
    "        \n",
    "    # Sauvegarder les données filtrées \n",
    "    df_filtered.to_excel(output_file_filtered, index=False)\n",
    "\n",
    "\n",
    "# Liste des fichiers Excel à traiter\n",
    "files = ['/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_1.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_2.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_2.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_3.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_4.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_5.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_6.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_leave_p1.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_quit.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/bluesky.xlsx']\n",
    "\n",
    "# Boucle principale pour traiter chaque fichier\n",
    "for file in files:\n",
    "    process_file(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fusion des tables et tri par date\n",
    "\n",
    "On fusionne les tables et on trie par date : on obtient finalement la base **tweets_fusionnes.xlsx**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_stay_1_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_exode_1_date.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_exode_2_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_last_2_date.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_last_3_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_last_4_date.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_last_5_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_last_6_date.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_last_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_leave_p1_date.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_nettoye/tweets_quit_date.xlsx'\n",
    "   \n",
    "]\n",
    "\n",
    "# Charger et fusionner tous les fichiers\n",
    "dataframes = []  # Liste pour stocker les DataFrames\n",
    "for file in file_list:\n",
    "    df = pd.read_excel(file)  # Charger chaque fichier\n",
    "    dataframes.append(df)  # Ajouter le DataFrame à la liste\n",
    "\n",
    "# Fusionner tous les DataFrames en un seul\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Suppression des bots\n",
    "\n",
    "Nous nous sommes rendus compte que certains tweets revenaient plusieurs fois, après avoir réfléchi et testé plusieurs façons de supprimer les bots, nous nous sommes arrêtés à la définition d'un bot comme un tweet identique qui revient 4 fois ou plus, et qui fait moins de 1000 vues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage terminé et fichiers sauvegardés.\n"
     ]
    }
   ],
   "source": [
    "df = merged_df\n",
    "\n",
    "# Compter les occurrences de chaque tweet\n",
    "content_counts = df['Content'].value_counts()\n",
    "\n",
    "# Filtrer les contenus apparaissant au moins 4 fois\n",
    "repeated_contents = content_counts[content_counts >= 4].index\n",
    "\n",
    "# Identifier les tweets répétitifs\n",
    "repeated_tweets = df[df['Content'].isin(repeated_contents)]\n",
    "\n",
    "# Identifier les groupes où un tweet a plus de 1000 vues\n",
    "to_keep = repeated_tweets.groupby('Content')['Views'].max()  # Max des vues pour chaque groupe\n",
    "valid_tweets = to_keep[to_keep > 1000].index  # Conserver les groupes avec au moins un tweet > 1000 vues\n",
    "\n",
    "# Séparer les tweets répétitifs en deux groupes\n",
    "# Tweets à conserver (au moins 1 tweet > 1000 vues dans le groupe)\n",
    "tweets_to_keep = repeated_tweets[repeated_tweets['Content'].isin(valid_tweets)]\n",
    "\n",
    "# Tweets à supprimer (aucun tweet > 1000 vues dans le groupe)\n",
    "tweets_to_remove = repeated_tweets[~repeated_tweets['Content'].isin(valid_tweets)]\n",
    "\n",
    "# Retirer explicitement les tweets à supprimer de la base originale\n",
    "df_cleaned = df[~df['Content'].isin(tweets_to_remove['Content'])]\n",
    "\n",
    "# Sauvegarder les tweets répétitifs supprimés\n",
    "#tweets_to_remove.to_excel('removed_repeated_tweets.xlsx', index=False)\n",
    "\n",
    "# Sauvegarder la base nettoyée\n",
    "df_cleaned.to_excel('/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/tweets_fusionnes.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour nettoyer la base de données\n",
    "def clean_tweets(df, output_cleaned):\n",
    "\n",
    "    # Compter les occurrences de chaque tweet\n",
    "    content_counts = df['Content'].value_counts()\n",
    "\n",
    "    # Filtrer les contenus apparaissant au moins 4 fois\n",
    "    repeated_contents = content_counts[content_counts >= 4].index\n",
    "\n",
    "    # Identifier les tweets répétitifs\n",
    "    repeated_tweets = df[df['Content'].isin(repeated_contents)]\n",
    "\n",
    "    # Identifier les groupes où un tweet a plus de 1000 vues\n",
    "    to_keep = repeated_tweets.groupby('Content')['Views'].max()  # Max des vues pour chaque groupe\n",
    "    valid_tweets = to_keep[to_keep > 1000].index  # Conserver les groupes avec au moins un tweet > 1000 vues\n",
    "\n",
    "    # Séparer les tweets répétitifs en deux groupes\n",
    "    # Tweets à conserver (au moins 1 tweet > 1000 vues dans le groupe)\n",
    "    tweets_to_keep = repeated_tweets[repeated_tweets['Content'].isin(valid_tweets)]\n",
    "\n",
    "    # Tweets à supprimer (aucun tweet > 1000 vues dans le groupe)\n",
    "    tweets_to_remove = repeated_tweets[~repeated_tweets['Content'].isin(valid_tweets)]\n",
    "\n",
    "    # Retirer explicitement les tweets à supprimer de la base originale\n",
    "    df_cleaned = df[~df['Content'].isin(tweets_to_remove['Content'])]\n",
    "\n",
    "    # Sauvegarder les tweets supprimés\n",
    "    #tweets_to_remove.to_excel(output_removed, index=False)\n",
    "\n",
    "    # Sauvegarder la base nettoyée\n",
    "    df_cleaned.to_excel(output_cleaned, index=False)\n",
    "\n",
    "# Charger les deux bases de données\n",
    "df1 = pd.read_excel('/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/tweets_fusionnes.xlsx')  \n",
    "df2 = pd.read_excel('/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/bluesky_nettoye.xlsx')  \n",
    "\n",
    "# Appliquer le nettoyage sur la première base\n",
    "clean_tweets(\n",
    "    df1,\n",
    "    output_cleaned='/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/tweets_fusionnes.xlsx'\n",
    ")\n",
    "\n",
    "# Appliquer le nettoyage sur la deuxième base\n",
    "clean_tweets(\n",
    "    df2,\n",
    "    output_cleaned='/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/bluesky_nettoye.xlsx'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trie toute la table nettoyée sans les bots par date pour avoir la base finale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que la colonne 'ConvertedDate' est bien en format datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(merged_df['ConvertedDate']):\n",
    "    merged_df['ConvertedDate'] = pd.to_datetime(merged_df['ConvertedDate'])\n",
    "\n",
    "# Trier par date décroissante\n",
    "merged_df = merged_df.sort_values(by='ConvertedDate', ascending=False)\n",
    "\n",
    "# Sauvegarder la table fusionnée et triée\n",
    "output_file = '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_fusionnes.xlsx'\n",
    "merged_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base triée sauvegardée dans : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/tweets_fusionnes.xlsx\n",
      "Base triée sauvegardée dans : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/bluesky_nettoye.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour trier et sauvegarder une base\n",
    "def sort_and_save_by_date(df, date_column, output_file):\n",
    "\n",
    "    # Vérifier que la colonne de date est bien au format datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_column]):\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "\n",
    "    # Trier par date décroissante\n",
    "    df_sorted = df.sort_values(by=date_column, ascending=False)\n",
    "\n",
    "    # Sauvegarder la table triée\n",
    "    df_sorted.to_excel(output_file, index=False)\n",
    "\n",
    "# Charger les deux bases de données\n",
    "df1 = pd.read_excel('/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/tweets_fusionnes.xlsx')  \n",
    "df2 = pd.read_excel('/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/bluesky_nettoye.xlsx') \n",
    "\n",
    "# Appliquer le tri et sauvegarder la première base\n",
    "sort_and_save_by_date(\n",
    "    df1,\n",
    "    date_column='ConvertedDate',\n",
    "    output_file='/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/tweets_fusionnes.xlsx'\n",
    ")\n",
    "\n",
    "# Appliquer le tri et sauvegarder la deuxième base\n",
    "sort_and_save_by_date(\n",
    "    df2,\n",
    "    date_column='ConvertedDate',\n",
    "    output_file='/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/data_fin/bluesky_nettoye.xlsx'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apparence de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Repost</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Views</th>\n",
       "      <th>ConvertedDate</th>\n",
       "      <th>YearWeek</th>\n",
       "      <th>YearMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>𝐇𝐎𝐓𝐄𝐏 悪いバナー</td>\n",
       "      <td>Nov 30</td>\n",
       "      <td>The left have become so dependent on Twitter that they're forced with two options:\\n\\n1) Leave Twitter and leave behind the one platform that they feel grants them so much power.\\n\\n2) Stay on Twitter but deal with Musk, MAGA, Trump, etc.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>2024-47</td>\n",
       "      <td>2024-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StraitjacketEnjoyer</td>\n",
       "      <td>Nov 30</td>\n",
       "      <td>Have not noticed anything out of the ordinary since Musk bought Twitter.\\n\\nSome people are annoying but you can always just block them.\\n\\nAny specific reason you will leave Twitter or is it just that you don't like the platform owner?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>2024-47</td>\n",
       "      <td>2024-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Juan Sánchez Villa-Lobos Ramírez</td>\n",
       "      <td>Nov 30</td>\n",
       "      <td>Change to Twitter Suggests Elon Musk Is Panicking Over Users Leaving for Bluesky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>2024-47</td>\n",
       "      <td>2024-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MercadoMagico.com USA</td>\n",
       "      <td>Nov 30</td>\n",
       "      <td>Change to Twitter Suggests Elon Musk Is Panicking Over Users Leaving for #Bluesky</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>2024-47</td>\n",
       "      <td>2024-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donel Adams</td>\n",
       "      <td>Nov 30</td>\n",
       "      <td>Change to Twitter Suggests Elon Musk Is Panicking Over Users Leaving for Bluesky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>2024-47</td>\n",
       "      <td>2024-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stop That. @stopthatgirl7@famichiki.jp</td>\n",
       "      <td>Nov 30</td>\n",
       "      <td>Change to Twitter Suggests Elon Musk Is Panicking Over Users Leaving for Bluesky</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>2024-47</td>\n",
       "      <td>2024-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Maliah Well known (Gone/Left)</td>\n",
       "      <td>Nov 30</td>\n",
       "      <td>I am Leaving Twitter Forever. But I'll be on Bluesky, it's way better than twitter. So to my friends or anyone who's following me, I'll only be active on Bluesky. I will never support Donald Trump, I despise that Fascist including Elon Musk. Goodbye Forever \"Twitter\". Hi Bluesky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>2024-47</td>\n",
       "      <td>2024-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Username    Date                                                                                                                                                                                                                                                                                  Content  Comments  Repost  Likes  Views ConvertedDate YearWeek YearMonth\n",
       "0                             𝐇𝐎𝐓𝐄𝐏 悪いバナー  Nov 30                                           The left have become so dependent on Twitter that they're forced with two options:\\n\\n1) Leave Twitter and leave behind the one platform that they feel grants them so much power.\\n\\n2) Stay on Twitter but deal with Musk, MAGA, Trump, etc.         1       0      0     49    2024-11-30  2024-47   2024-11\n",
       "1                     StraitjacketEnjoyer  Nov 30                                             Have not noticed anything out of the ordinary since Musk bought Twitter.\\n\\nSome people are annoying but you can always just block them.\\n\\nAny specific reason you will leave Twitter or is it just that you don't like the platform owner?         0       0      0     47    2024-11-30  2024-47   2024-11\n",
       "2        Juan Sánchez Villa-Lobos Ramírez  Nov 30                                                                                                                                                                                                         Change to Twitter Suggests Elon Musk Is Panicking Over Users Leaving for Bluesky         0       0      0     17    2024-11-30  2024-47   2024-11\n",
       "3                   MercadoMagico.com USA  Nov 30                                                                                                                                                                                                        Change to Twitter Suggests Elon Musk Is Panicking Over Users Leaving for #Bluesky         0       2      0     43    2024-11-30  2024-47   2024-11\n",
       "4                             Donel Adams  Nov 30                                                                                                                                                                                                         Change to Twitter Suggests Elon Musk Is Panicking Over Users Leaving for Bluesky         0       0      0     22    2024-11-30  2024-47   2024-11\n",
       "5  Stop That. @stopthatgirl7@famichiki.jp  Nov 30                                                                                                                                                                                                         Change to Twitter Suggests Elon Musk Is Panicking Over Users Leaving for Bluesky         1       0      2     44    2024-11-30  2024-47   2024-11\n",
       "6           Maliah Well known (Gone/Left)  Nov 30  I am Leaving Twitter Forever. But I'll be on Bluesky, it's way better than twitter. So to my friends or anyone who's following me, I'll only be active on Bluesky. I will never support Donald Trump, I despise that Fascist including Elon Musk. Goodbye Forever \"Twitter\". Hi Bluesky         0       0      0     39    2024-11-30  2024-47   2024-11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on modifie les paramètres pour afficher plus de contenu dans chaque cellule\n",
    "pd.set_option('display.max_colwidth', None)  # Affiche tout le contenu des colonnes\n",
    "pd.set_option('display.max_columns', None)  # Affiche toutes les colonnes\n",
    "pd.set_option('display.width', 1000)  # Ajuste la largeur totale de l'affichage\n",
    "\n",
    "# on charge et affiche le fichier Excel\n",
    "df = pd.read_excel('tweets_fusionnes.xlsx')\n",
    "display(df.head(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nettoyage des tweets sur le sujet \"Bluesky\"\n",
    "\n",
    "Après avoir récupéré les tweets sur le sujet \"Bluesky\" dans un tableau de données, on va simplement supprimer les tweets qui ne mentionnent pas directement le terme \"bluesky\" (ce dernier peut en effet faire partie du nom d'utilisateur et donc avoir été récupéré sans pour autant être en rapport avec le sujet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
