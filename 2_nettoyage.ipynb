{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage des données\n",
    "\n",
    "Après avoir scrappé, il nous faut tenir compte de plusieurs choses pour nettoyer les bases de données et les fusionner :\n",
    "- dans un premier temps, on convertit les dates dans un format date permettant ensuite de trier et de réarranger la table dans le bon ordre temporel.\n",
    "- les cases vides susceptibles d'apparaître dans les colonnes \"Likes\", \"Views\", etc... sont remplacées par des 0 (il n'y a eu en effet aucun like par exemple).\n",
    "- on fusionne les tables et on les trie par date\n",
    "- enfin, on supprime les bots, définis dans notre code par une répétition de 4 tweets faisant moins de 1000 vues (car certains tweets qui se répètent peuvent être des titres d'article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: datetime in /opt/conda/lib/python3.12/site-packages (5.5)\n",
      "Requirement already satisfied: zope.interface in /opt/conda/lib/python3.12/site-packages (from datetime) (7.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from datetime) (2024.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from zope.interface->datetime) (75.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas \n",
    "!pip install datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convertir les dates en format date\n",
    "\n",
    "Dans le code suivant, nous cherchons d'une part à remplacer les 2.3K par 2300, à convertir les dates dans un format *datetime*.\n",
    "\n",
    "Nous avons fait le choix d'ajouter deux colonnes :\n",
    "- une colonne YearWeek sous format 2024-1 pour la première semaine de 2024\n",
    "- une colonne YearMonth sous format 2024-10 pour le mois d'octobre 2024\n",
    "\n",
    "Nous avons décidé de nous restreindre à l'année 2024, en excluant le mois de décembre (car pas complet) : en effet, les fluctuations de 2023 étaient relativement similaires à celles de début de début 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1.xlsx\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Boucle principale pour traiter chaque fichier\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraitement du fichier : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Charger les données\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Nettoyer les colonnes numériques\u001b[39;00m\n\u001b[1;32m     60\u001b[0m colonnes_a_nettoyer \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRepost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLikes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViews\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Fonction 1 : Nettoyer les données numériques\n",
    "def convertir_en_nombre(valeur):\n",
    "    \"\"\"\n",
    "    Convertit les chaînes de caractères (ex: '1.2K') en nombres.\n",
    "    Remplace NaN par 0.\n",
    "    \"\"\"\n",
    "    if pd.isna(valeur):  # Si la valeur est NaN, retourne 0\n",
    "        return 0\n",
    "    if isinstance(valeur, str):  # Vérifie si la valeur est une chaîne de caractères\n",
    "        valeur = valeur.strip()  # Supprime les espaces inutiles\n",
    "        if 'K' in valeur:  # Si la valeur contient 'K', multiplier par 1 000\n",
    "            return float(valeur.replace('K', '')) * 1000\n",
    "        elif 'M' in valeur:  # Si la valeur contient 'M', multiplier par 1 000 000\n",
    "            return float(valeur.replace('M', '')) * 1000000\n",
    "    try:\n",
    "        return float(valeur)  # Sinon, convertir directement en nombre\n",
    "    except ValueError:\n",
    "        return 0  # Si la conversion échoue, retourner 0\n",
    "\n",
    "# Fonction 2 : Conversion des dates\n",
    "def convert_date(value, previous_date):\n",
    "    \"\"\"\n",
    "    Convertit les valeurs de date relative ou absolue en objets datetime.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = str(value).strip()  # S'assurer que la valeur est une chaîne\n",
    "        \n",
    "        # Cas 1 : Format \"Nov 5\" (pas d'année explicite)\n",
    "        if len(value.split()) == 2:\n",
    "            value_with_year = value + \" 2024\"  # Ajouter l'année par défaut\n",
    "            return datetime.strptime(value_with_year, '%b %d %Y')\n",
    "        \n",
    "        # Cas 2 : Format complet \"Nov 5, 2023\"\n",
    "        elif len(value.split()) == 3:\n",
    "            return datetime.strptime(value, '%b %d, %Y')\n",
    "        \n",
    "        # Cas 3 : Valeur non reconnue\n",
    "        else:\n",
    "            return pd.NaT\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la conversion de la date : {value} -> {e}\")\n",
    "        return pd.NaT  # Retourne NaT si la conversion échoue\n",
    "\n",
    "# Fonction 3 : Nettoyer et transformer un fichier\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Nettoie et transforme un fichier Excel, en ajoutant des colonnes converties\n",
    "    et en générant deux fichiers de sortie :\n",
    "    1. Toutes les données transformées.\n",
    "    2. Données filtrées avant une date spécifique.\n",
    "    \"\"\"\n",
    "    print(f\"Traitement du fichier : {file_path}\")\n",
    "    # Charger les données\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Nettoyer les colonnes numériques\n",
    "    colonnes_a_nettoyer = ['Comments', 'Repost', 'Likes', 'Views']\n",
    "    for colonne in colonnes_a_nettoyer:\n",
    "        if colonne in df.columns:\n",
    "            df[colonne] = df[colonne].apply(convertir_en_nombre)\n",
    "    \n",
    "    # Convertir les dates\n",
    "    converted_dates = []\n",
    "    previous_date = None\n",
    "    for value in df['Date']:\n",
    "        if previous_date is None:  # La première ligne doit être une date absolue\n",
    "            converted_date = convert_date(value, datetime(2024, 1, 1))  # Supposition du début\n",
    "        else:\n",
    "            converted_date = convert_date(value, previous_date)\n",
    "        converted_dates.append(converted_date)\n",
    "        if pd.notna(converted_date):  # Mettre à jour la référence seulement si la conversion a réussi\n",
    "            previous_date = converted_date\n",
    "    \n",
    "    # Ajouter les colonnes converties\n",
    "    df['ConvertedDate'] = converted_dates\n",
    "    df['YearWeek'] = df['ConvertedDate'].dt.strftime('%Y-%U')\n",
    "    df['YearMonth'] = df['ConvertedDate'].dt.strftime('%Y-%m')\n",
    "    \n",
    "\n",
    "\n",
    "    # Filtrer les données pour garder uniquement les dates entre le 1er janv et le 1er décembre 2024\n",
    "    # Définir les bornes de la période\n",
    "    start_date = datetime(2024, 1, 1)  # 1er janvier 2024 inclus\n",
    "    end_date = datetime(2024, 11, 30)  # 30 novembre 2024 inclus\n",
    "\n",
    "    # Appliquer le filtre pour garder les dates entre ces bornes\n",
    "    df_filtered = df[(df['ConvertedDate'] >= start_date) & (df['ConvertedDate'] <= end_date)]\n",
    "\n",
    "    \n",
    "    # Sauvegarder les données filtrées : on renomme les fichiers en ajoutant _date à la fin\n",
    "    output_file_filtered = file_path.replace('.xlsx', '_date.xlsx')\n",
    "    df_filtered.to_excel(output_file_filtered, index=False)\n",
    "\n",
    "\n",
    "# Liste des fichiers Excel à traiter\n",
    "files = ['/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_1.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_2.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_2.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_3.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_4.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_5.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_6.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_leave_p1.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_quit.xlsx']\n",
    "\n",
    "# Boucle principale pour traiter chaque fichier\n",
    "for file in files:\n",
    "    process_file(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fusion des tables et tri par date\n",
    "\n",
    "On fusionne les tables et on trie par date : on arrive au fichier tweets_fusionnes.xlsx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_list = [\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_1_date.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_2_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_2_date.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_3_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_4_date.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_5_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_6_date.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_leave_p1_date.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_quit_date.xlsx'\n",
    "   \n",
    "]\n",
    "\n",
    "# Charger et fusionner tous les fichiers\n",
    "dataframes = []  # Liste pour stocker les DataFrames\n",
    "for file in file_list:\n",
    "    df = pd.read_excel(file)  # Charger chaque fichier\n",
    "    dataframes.append(df)  # Ajouter le DataFrame à la liste\n",
    "\n",
    "# Fusionner tous les DataFrames en un seul\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Suppression des bots\n",
    "\n",
    "Nous nous sommes rendus compte que certains tweets revenaient plusieurs fois, après avoir réfléchi et testé plusieurs façons de supprimer les bots, nous nous sommes arrêtés à la définition d'un bot comme un tweet identique qui revient 4 fois ou plus, et qui fait moins de 1000 vues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df\n",
    "\n",
    "# Vérifier que les colonnes nécessaires existent\n",
    "if 'Content' not in df.columns or 'Views' not in df.columns:\n",
    "    raise ValueError(\"Les colonnes 'Content' et 'Views' sont nécessaires pour ce traitement.\")\n",
    "\n",
    "# Compter les occurrences de chaque tweet\n",
    "content_counts = df['Content'].value_counts()\n",
    "\n",
    "# Filtrer les contenus apparaissant au moins 4 fois\n",
    "repeated_contents = content_counts[content_counts >= 4].index\n",
    "\n",
    "# Identifier les tweets répétitifs\n",
    "repeated_tweets = df[df['Content'].isin(repeated_contents)]\n",
    "\n",
    "# Identifier les groupes où un tweet a plus de 1000 vues\n",
    "to_keep = repeated_tweets.groupby('Content')['Views'].max()  # Max des vues pour chaque groupe\n",
    "valid_tweets = to_keep[to_keep > 1000].index  # Conserver les groupes avec au moins un tweet > 1000 vues\n",
    "\n",
    "# Séparer les tweets répétitifs en deux groupes\n",
    "# Tweets à conserver (au moins 1 tweet > 1000 vues dans le groupe)\n",
    "tweets_to_keep = repeated_tweets[repeated_tweets['Content'].isin(valid_tweets)]\n",
    "\n",
    "# Tweets à supprimer (aucun tweet > 1000 vues dans le groupe)\n",
    "tweets_to_remove = repeated_tweets[~repeated_tweets['Content'].isin(valid_tweets)]\n",
    "\n",
    "# Combiner les données finales\n",
    "df_cleaned = pd.concat([df[~df['Content'].isin(repeated_contents)], tweets_to_keep])\n",
    "\n",
    "# Sauvegarder les tweets répétitifs supprimés\n",
    "# tweets_to_remove.to_excel('removed_repeated_tweets.xlsx', index=False)\n",
    "\n",
    "# Sauvegarder la base nettoyée\n",
    "df_cleaned.to_excel('/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_fusionnes.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trie toute la table nettoyée sans les bots par date pour avoir la base finale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que la colonne 'ConvertedDate' est bien en format datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(merged_df['ConvertedDate']):\n",
    "    merged_df['ConvertedDate'] = pd.to_datetime(merged_df['ConvertedDate'])\n",
    "\n",
    "# Trier par date décroissante\n",
    "merged_df = merged_df.sort_values(by='ConvertedDate', ascending=False)\n",
    "\n",
    "# Sauvegarder la table fusionnée et triée\n",
    "output_file = '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_fusionnes.xlsx'\n",
    "merged_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Jointure avec les résultats du NLP\n",
    "\n",
    "On a une colonne en plus avec :\n",
    "- 0 : le tweet n'est pas violent\n",
    "- 1 : le tweet est violent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Il n'y a pas le même nombre de lignes dans les deux dataframes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m df_date \u001b[38;5;241m=\u001b[39m df_date\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_date) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_nlp):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIl n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my a pas le même nombre de lignes dans les deux dataframes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m df_fusion \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_date, df_nlp], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m df_fusion\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtables_fusion.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Il n'y a pas le même nombre de lignes dans les deux dataframes"
     ]
    }
   ],
   "source": [
    "read_file = pd.read_excel(\"tweets_fusionnes.xlsx\")\n",
    "read_file.to_csv(\"tweets_fusionnes.csv\", index = None, header = True) #conversion Excel en CSV\n",
    "\n",
    "df_date = pd.read_csv(\"tweets_fusionnes.csv\")\n",
    "df_nlp = pd.read_csv(\"labeled_data_with_predictions.csv\")\n",
    "\n",
    "df_date = df_date.drop('Date', axis=1)\n",
    "\n",
    "if len(df_date) != len(df_nlp):\n",
    "    raise ValueError(\"Il n'y a pas le même nombre de lignes dans les deux dataframes\")\n",
    "\n",
    "df_fusion = pd.concat([df_date, df_nlp], axis=1)\n",
    "df_fusion.to_csv('tables_fusion.csv', index = False)\n",
    "\n",
    "print(df_fusion.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
