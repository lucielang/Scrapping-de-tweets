{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage des données\n",
    "\n",
    "Après avoir scrappé, il nous faut tenir compte de plusieurs choses pour nettoyer les bases de données et les fusionner :\n",
    "- dans un premier temps, on convertit les dates dans un format date permettant ensuite de trier et de réarranger la table dans le bon ordre temporel.\n",
    "- les cases vides susceptibles d'apparaître dans les colonnes \"Likes\", \"Views\", etc... sont remplacées par des 0 (il n'y a eu en effet aucun like par exemple).\n",
    "- on fusionne les tables et on les trie par date\n",
    "- enfin, on supprime les bots, définis dans notre code par une répétition de 4 tweets faisant moins de 1000 vues (car certains tweets qui se répètent peuvent être des titres d'article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: datetime in /opt/conda/lib/python3.12/site-packages (5.5)\n",
      "Requirement already satisfied: zope.interface in /opt/conda/lib/python3.12/site-packages (from datetime) (7.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from datetime) (2024.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from zope.interface->datetime) (75.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convertir les dates en format date\n",
    "\n",
    "Dans le code suivant, nous cherchons d'une part à remplacer les 2.3K par 2300, à convertir les dates dans un format *datetime*.\n",
    "\n",
    "Nous avons fait le choix d'ajouter deux colonnes :\n",
    "- une colonne YearWeek sous format 2024-1 pour la première semaine de 2024\n",
    "- une colonne YearMonth sous format 2024-10 pour le mois d'octobre 2024\n",
    "\n",
    "Nous avons décidé de nous restreindre à l'année 2024, en excluant le mois de décembre (car pas complet) : en effet, les fluctuations de 2023 étaient relativement similaires à celles de début de début 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_1.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_2.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_2.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_3.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_4.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_5.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_6.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_leave_p1.xlsx\n",
      "Traitement du fichier : /home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_quit.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Fonction 1 : Nettoyer les données numériques\n",
    "def convertir_en_nombre(valeur):\n",
    "    \"\"\"\n",
    "    Convertit les chaînes de caractères (ex: '1.2K') en nombres.\n",
    "    Remplace NaN par 0.\n",
    "    \"\"\"\n",
    "    if pd.isna(valeur):  # Si la valeur est NaN, retourne 0\n",
    "        return 0\n",
    "    if isinstance(valeur, str):  # Vérifie si la valeur est une chaîne de caractères\n",
    "        valeur = valeur.strip()  # Supprime les espaces inutiles\n",
    "        if 'K' in valeur:  # Si la valeur contient 'K', multiplier par 1 000\n",
    "            return float(valeur.replace('K', '')) * 1000\n",
    "        elif 'M' in valeur:  # Si la valeur contient 'M', multiplier par 1 000 000\n",
    "            return float(valeur.replace('M', '')) * 1000000\n",
    "    try:\n",
    "        return float(valeur)  # Sinon, convertir directement en nombre\n",
    "    except ValueError:\n",
    "        return 0  # Si la conversion échoue, retourner 0\n",
    "\n",
    "# Fonction 2 : Conversion des dates\n",
    "def convert_date(value, previous_date):\n",
    "    \"\"\"\n",
    "    Convertit les valeurs de date relative ou absolue en objets datetime.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = str(value).strip()  # S'assurer que la valeur est une chaîne\n",
    "        \n",
    "        # Cas 1 : Format \"Nov 5\" (pas d'année explicite)\n",
    "        if len(value.split()) == 2:\n",
    "            value_with_year = value + \" 2024\"  # Ajouter l'année par défaut\n",
    "            return datetime.strptime(value_with_year, '%b %d %Y')\n",
    "        \n",
    "        # Cas 2 : Format complet \"Nov 5, 2023\"\n",
    "        elif len(value.split()) == 3:\n",
    "            return datetime.strptime(value, '%b %d, %Y')\n",
    "        \n",
    "        # Cas 3 : Valeur non reconnue\n",
    "        else:\n",
    "            return pd.NaT\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la conversion de la date : {value} -> {e}\")\n",
    "        return pd.NaT  # Retourne NaT si la conversion échoue\n",
    "\n",
    "# Fonction 3 : Nettoyer et transformer un fichier\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Nettoie et transforme un fichier Excel, en ajoutant des colonnes converties\n",
    "    et en générant deux fichiers de sortie :\n",
    "    1. Toutes les données transformées.\n",
    "    2. Données filtrées avant une date spécifique.\n",
    "    \"\"\"\n",
    "    print(f\"Traitement du fichier : {file_path}\")\n",
    "    # Charger les données\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Nettoyer les colonnes numériques\n",
    "    colonnes_a_nettoyer = ['Comments', 'Repost', 'Likes', 'Views']\n",
    "    for colonne in colonnes_a_nettoyer:\n",
    "        if colonne in df.columns:\n",
    "            df[colonne] = df[colonne].apply(convertir_en_nombre)\n",
    "    \n",
    "    # Convertir les dates\n",
    "    converted_dates = []\n",
    "    previous_date = None\n",
    "    for value in df['Date']:\n",
    "        if previous_date is None:  # La première ligne doit être une date absolue\n",
    "            converted_date = convert_date(value, datetime(2024, 1, 1))  # Supposition du début\n",
    "        else:\n",
    "            converted_date = convert_date(value, previous_date)\n",
    "        converted_dates.append(converted_date)\n",
    "        if pd.notna(converted_date):  # Mettre à jour la référence seulement si la conversion a réussi\n",
    "            previous_date = converted_date\n",
    "    \n",
    "    # Ajouter les colonnes converties\n",
    "    df['ConvertedDate'] = converted_dates\n",
    "    df['YearWeek'] = df['ConvertedDate'].dt.strftime('%Y-%U')\n",
    "    df['YearMonth'] = df['ConvertedDate'].dt.strftime('%Y-%m')\n",
    "    \n",
    "\n",
    "    # Filtrer les données pour garder uniquement les dates avant le 1er décembre 2024\n",
    "    cutoff_date = datetime(2024, 12, 1)\n",
    "    df_filtered = df[df['ConvertedDate'] < cutoff_date]\n",
    "    \n",
    "    # Sauvegarder les données filtrées : on renomme les fichiers en ajoutant _date à la fin\n",
    "    output_file_filtered = file_path.replace('.xlsx', '_date.xlsx')\n",
    "    df_filtered.to_excel(output_file_filtered, index=False)\n",
    "\n",
    "\n",
    "# Liste des fichiers Excel à traiter\n",
    "files = ['/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_1.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_2.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_2.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_3.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_4.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_5.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_6.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_leave_p1.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_quit.xlsx']\n",
    "\n",
    "# Boucle principale pour traiter chaque fichier\n",
    "for file in files:\n",
    "    process_file(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fusion des tables et tri par date\n",
    "\n",
    "On fusionne les tables et on trie par date : on arrive au fichier tweets_fusionnes.xlsx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_list = [\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_stay_1_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_1_date.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_exode_2_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_2_date.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_3_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_4_date.xlsx',\n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_5_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_6_date.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_last_date.xlsx', '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_leave_p1_date.xlsx', \n",
    "'/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_scrap/tweets_quit_date.xlsx'\n",
    "\n",
    "   \n",
    "]\n",
    "\n",
    "# Charger et fusionner tous les fichiers\n",
    "dataframes = []  # Liste pour stocker les DataFrames\n",
    "for file in file_list:\n",
    "    df = pd.read_excel(file)  # Charger chaque fichier\n",
    "    dataframes.append(df)  # Ajouter le DataFrame à la liste\n",
    "\n",
    "# Fusionner tous les DataFrames en un seul\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# Vérifier que la colonne 'ConvertedDate' est bien en format datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(merged_df['ConvertedDate']):\n",
    "    merged_df['ConvertedDate'] = pd.to_datetime(merged_df['ConvertedDate'])\n",
    "\n",
    "# Trier par date décroissante\n",
    "merged_df = merged_df.sort_values(by='ConvertedDate', ascending=False)\n",
    "\n",
    "# Sauvegarder la table fusionnée et triée\n",
    "output_file = '/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_fusionnes.xlsx'\n",
    "merged_df.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Suppression des bots\n",
    "\n",
    "Nous nous sommes rendus compte que certains tweets revenaient plusieurs fois, après avoir réfléchi et testé plusieurs façons de supprimer les bots, nous nous sommes arrêtés à la définition d'un bot comme un tweet identique qui revient 4 fois ou plus, et qui fait moins de 1000 vues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_fusionnes.xlsx')\n",
    "\n",
    "# Vérifier que les colonnes nécessaires existent\n",
    "if 'Content' not in df.columns or 'Views' not in df.columns:\n",
    "    raise ValueError(\"Les colonnes 'Content' et 'Views' sont nécessaires pour ce traitement.\")\n",
    "\n",
    "# Compter les occurrences de chaque tweet\n",
    "content_counts = df['Content'].value_counts()\n",
    "\n",
    "# Filtrer les contenus apparaissant au moins 4 fois\n",
    "repeated_contents = content_counts[content_counts >= 4].index\n",
    "\n",
    "# Identifier les tweets répétitifs\n",
    "repeated_tweets = df[df['Content'].isin(repeated_contents)]\n",
    "\n",
    "# Identifier les groupes où un tweet a plus de 1000 vues\n",
    "to_keep = repeated_tweets.groupby('Content')['Views'].max()  # Max des vues pour chaque groupe\n",
    "valid_tweets = to_keep[to_keep > 1000].index  # Conserver les groupes avec au moins un tweet > 1000 vues\n",
    "\n",
    "# Séparer les tweets répétitifs en deux groupes\n",
    "# Tweets à conserver (au moins 1 tweet > 1000 vues dans le groupe)\n",
    "tweets_to_keep = repeated_tweets[repeated_tweets['Content'].isin(valid_tweets)]\n",
    "\n",
    "# Tweets à supprimer (aucun tweet > 1000 vues dans le groupe)\n",
    "tweets_to_remove = repeated_tweets[~repeated_tweets['Content'].isin(valid_tweets)]\n",
    "\n",
    "# Combiner les données finales\n",
    "df_cleaned = pd.concat([df[~df['Content'].isin(repeated_contents)], tweets_to_keep])\n",
    "\n",
    "# Sauvegarder les tweets répétitifs supprimés\n",
    "# tweets_to_remove.to_excel('removed_repeated_tweets.xlsx', index=False)\n",
    "\n",
    "# Sauvegarder la base nettoyée\n",
    "df_cleaned.to_excel('/home/onyxia/work/Scrapping_tweets/Scrapping_tweets/tweets_fusionnes.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Jointure avec les résultats du NLP\n",
    "\n",
    "On a une colonne en plus avec :\n",
    "- 0 : le tweet n'est pas violent\n",
    "- 1 : le tweet est violent "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
